{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - RAG-Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we create a chatbot using RAG ([Retrieval Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)) and [GraphRAG](https://graphrag.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our usual suspects (and some more...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from neo4j import Query, GraphDatabase, RoutingControl, Result\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "from json import loads, dumps\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_file = 'ws.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(env_file):\n",
    "    load_dotenv(env_file, override=True)\n",
    "\n",
    "    # Neo4j\n",
    "    HOST = os.getenv('NEO4J_URI')\n",
    "    USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "    PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "    DATABASE = os.getenv('NEO4J_DATABASE')\n",
    "\n",
    "    # AI\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    os.environ['OPENAI_API_KEY']=OPENAI_API_KEY\n",
    "    LLM = os.getenv('LLM')\n",
    "    EMBEDDINGS_MODEL = os.getenv('EMBEDDINGS_MODEL')\n",
    "else:\n",
    "    print(f\"File {env_file} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Connection to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup connection to the database with the [Python Driver](https://neo4j.com/docs/python-manual/5/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\n",
    "    HOST,\n",
    "    auth=(USERNAME, PASSWORD)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_query(\n",
    "    \"\"\"\n",
    "    MATCH (n) RETURN COUNT(n) as Count\n",
    "    \"\"\",\n",
    "    database_=DATABASE,\n",
    "    routing_=RoutingControl.READ,\n",
    "    result_transformer_= lambda r: r.to_df()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAG-application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the the chatbot we both need an Embedding-model and LLM. Create both below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=EMBEDDINGS_MODEL,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the difference between a \"Regular\" Vector Search and GraphRAG we create different retrieval queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function retrieves the context using a regular vector search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_vector_search(search_prompt):\n",
    "    query_vector = embedding_model.embed_query(search_prompt)\n",
    "    \n",
    "    similarity_query = \"\"\" \n",
    "        CALL db.index.vector.queryNodes(\"chunk-embeddings\", 3, $query_vector) YIELD node, score\n",
    "        WITH node as chunk, score ORDER BY score DESC\n",
    "        MATCH (d:Document)<-[:PART_OF]-(chunk)\n",
    "        RETURN score, d.file_name as file_name, chunk.id as chunk_id, chunk.page as page, chunk.chunk_eng AS chunk\n",
    "       \"\"\"\n",
    "    results = driver.execute_query(\n",
    "        similarity_query,\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        query_vector=query_vector,\n",
    "        result_transformer_= lambda r: r.to_df()\n",
    "    )\n",
    "    \n",
    "    results = results.to_json(orient=\"records\")\n",
    "    parsed = loads(results)\n",
    "    context = dumps(parsed, indent=2)\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function retrieves the context using a the Knowledge Graph (GraphRAG). We start with a regular vector search and can find more than that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_graphrag(search_prompt):\n",
    "    query_vector = embedding_model.embed_query(search_prompt)\n",
    "    \n",
    "    similarity_query = \"\"\" \n",
    "        CALL db.index.vector.queryNodes(\"chunk-embeddings\", 3, $query_vector) YIELD node, score\n",
    "        WITH node as chunk, score ORDER BY score DESC\n",
    "        CALL (chunk) {\n",
    "            MATCH (chunk)-[r:OVERLAPPING_DEFINITIONS]-(overlapping_chunk:Chunk)\n",
    "            WHERE r.overlap > 3\n",
    "            RETURN collect(overlapping_chunk) AS overlapping_chunks\n",
    "        }\n",
    "        WITH [chunk] + overlapping_chunks AS chunks\n",
    "        UNWIND chunks as chunk\n",
    "        MATCH (d:Document)<-[:PART_OF]-(chunk)\n",
    "        RETURN d.file_name as file_name, chunk.id as chunk_id, chunk.page as page, chunk.chunk_eng AS chunk\n",
    "       \"\"\"\n",
    "    results_1 = driver.execute_query(\n",
    "        similarity_query,\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        query_vector=query_vector,\n",
    "        result_transformer_= lambda r: r.to_df()\n",
    "    )\n",
    "\n",
    "    chunk_ids = list(set(results_1['chunk_id'].to_list()))\n",
    "\n",
    "    definition_query = \"\"\"    \n",
    "       CALL db.index.vector.queryNodes(\"definition-embeddings\", 5, $query_vector) YIELD node, score\n",
    "            WITH node as definition, score ORDER BY score DESC\n",
    "            WHERE definition.degree < 20\n",
    "            WITH definition LIMIT 1\n",
    "            MATCH (definition)<-[:MENTIONS]-(chunk:Chunk)\n",
    "            WHERE NOT (chunk.id IN $chunk_ids)\n",
    "            WITH chunk LIMIT 3\n",
    "            MATCH (d:Document)<-[:PART_OF]-(chunk)\n",
    "            RETURN d.file_name as file_name, chunk.id as chunk_id, chunk.page as page, chunk.chunk_eng AS chunk\n",
    "    \"\"\"\n",
    "    results_2 = driver.execute_query(\n",
    "        definition_query,\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        chunk_ids=chunk_ids,\n",
    "        query_vector=query_vector,\n",
    "        result_transformer_= lambda r: r.to_df()\n",
    "    )\n",
    "    results = pd.concat([results_1,results_2]).drop_duplicates()\n",
    "    results = results.to_json(orient=\"records\")\n",
    "    parsed = loads(results)\n",
    "    context = dumps(parsed, indent=2)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt for vector search which returns just the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(search_prompt, context):\n",
    "    prompt_template = \"\"\"\n",
    "\n",
    "    You are a chatbot on Rabobank product. Your goal is to help people with questions on product policies.  \n",
    "    A user will come to you with questions on their policy. Their questions must be answered based on the relevant documents of the policy.\n",
    "    Respond in English. \n",
    "\n",
    "    The question is the following: \n",
    "    {search_prompt}\n",
    "    \n",
    "    Always respond in the language in which the question was asked. So, do not respond in a different language.\n",
    "    \n",
    "    The context is the following: \n",
    "    {context}\n",
    "\n",
    "    Please explain your answer as thorough as possbile based on the context above. Don't come up with anything yourself.\n",
    "    \n",
    "    Please end your message with listing your sources with file name and page number. \n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    \n",
    "    theprompt = prompt.format_prompt(search_prompt=search_prompt, context=context)\n",
    "    return theprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt for GraphRAG provides context and definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples to test the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every example there can be chosen between GraphRAG and vector search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prompt = 'What is meant with the Rabofoon?'\n",
    "\n",
    "context = get_context_vector_search(search_prompt)\n",
    "theprompt = generate_prompt(search_prompt, context)\n",
    "llm(theprompt.to_messages()).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prompt = 'What is meant with the Rabofoon?'\n",
    "\n",
    "context = get_context_graphrag(search_prompt)\n",
    "theprompt = generate_prompt(search_prompt, context)\n",
    "llm(theprompt.to_messages()).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prompt = 'Are you insured when traveling to a high-risk country?'\n",
    "\n",
    "context = get_context_vector_search(search_prompt)\n",
    "theprompt = generate_prompt(search_prompt, context)\n",
    "llm(theprompt.to_messages()).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prompt = 'Are you insured when traveling to a high-risk country?'\n",
    "\n",
    "context = get_context_graphrag(search_prompt)\n",
    "theprompt = generate_prompt(search_prompt, context)\n",
    "llm(theprompt.to_messages()).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prompt = 'What are the rules for a joint investment account?'\n",
    "\n",
    "context = get_context_vector_search(search_prompt)\n",
    "theprompt = generate_prompt(search_prompt, context)\n",
    "llm(theprompt.to_messages()).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prompt = 'What are the rules for a joint investment account?'\n",
    "\n",
    "context = get_context_graphrag(search_prompt)\n",
    "theprompt = generate_prompt(search_prompt, context)\n",
    "llm(theprompt.to_messages()).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Chatbot that uses RAG and GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code is coming from Gradio documentation: [Creating a custom chatbot with blocks](https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#add-streaming-to-your-chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user(user_message, history):\n",
    "    return \"\", history + [[user_message, None]]\n",
    "\n",
    "def get_answer(search_prompt, rag_method):\n",
    "    if rag_method == \"Vector-Search\":\n",
    "        context = get_context_vector_search(search_prompt)\n",
    "        theprompt = generate_prompt(search_prompt, context)\n",
    "    else: \n",
    "    # rag_method == \"GraphRAG\"\n",
    "        context = get_context_graphrag(search_prompt)\n",
    "        theprompt = generate_prompt(search_prompt, context)\n",
    "    messages = llm(theprompt.to_messages())\n",
    "    return messages.content\n",
    "\n",
    "def bot(history, rag_method):\n",
    "    bot_message = get_answer(history[-1][0], rag_method)\n",
    "    history[-1][1] = \"\"\n",
    "    for character in bot_message:\n",
    "        history[-1][1] += character\n",
    "        time.sleep(0.01)\n",
    "        yield history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"Chatbot with RAG\", \n",
    "        avatar_images=[\"https://png.pngtree.com/png-vector/20220525/ourmid/pngtree-concept-of-facial-animal-avatar-chatbot-dog-chat-machine-illustration-vector-png-image_46652864.jpg\",\"https://d-cb.jc-cdn.com/sites/crackberry.com/files/styles/larger/public/article_images/2023/08/openai-logo.jpg\"]\n",
    "    )\n",
    "    msg = gr.Textbox(label=\"Message\")\n",
    "    rag_method = gr.Radio([\"Vector-Search\", \"GraphRAG\"], label=\"RAG-method:\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, [chatbot, rag_method], chatbot\n",
    "    )\n",
    "    \n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    \n",
    "demo.queue()\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have the light-mode for the chatbot paste the following after the URL: /?__theme=light"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1712323594898,
  "creator": "admin",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "modifiedBy": "admin",
  "tags": [],
  "versionNumber": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
