{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 - GraphRAG and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook are we creating an agent on top of the Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our usual suspects (and some more...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j import Query, GraphDatabase, RoutingControl, Result\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from openai import OpenAI\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import functools\n",
    "from langchain_core.tools import tool\n",
    "import gradio as gr\n",
    "import time\n",
    "from json import loads, dumps\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, Graph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynPe6RLRWSKd"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa61u1jfyk3t"
   },
   "source": [
    "Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_file = 'ws.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHR_0lmElZ-R"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(env_file):\n",
    "    load_dotenv(env_file, override=True)\n",
    "\n",
    "    # Neo4j\n",
    "    HOST = os.getenv('NEO4J_URI')\n",
    "    USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "    PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "    DATABASE = os.getenv('NEO4J_DATABASE')\n",
    "\n",
    "    # AI\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    os.environ['OPENAI_API_KEY']=OPENAI_API_KEY\n",
    "    LLM = os.getenv('LLM')\n",
    "    EMBEDDINGS_MODEL = os.getenv('EMBEDDINGS_MODEL')\n",
    "else:\n",
    "    print(f\"File {env_file} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup connection to the database with the [Python Driver](https://neo4j.com/docs/python-manual/5/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\n",
    "    HOST,\n",
    "    auth=(USERNAME, PASSWORD)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5w4eCb7xZZ-S"
   },
   "outputs": [],
   "source": [
    "driver.execute_query(\n",
    "    \"\"\"\n",
    "    MATCH (n) RETURN COUNT(n) as Count\n",
    "    \"\"\",\n",
    "    database_=DATABASE,\n",
    "    routing_=RoutingControl.READ,\n",
    "    result_transformer_= lambda r: r.to_df()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdTfdAyV2ZaR"
   },
   "source": [
    "Test whether we got our constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdTfdAyV2ZaR"
   },
   "outputs": [],
   "source": [
    "schema_result_df  = driver.execute_query(\n",
    "    'show indexes',\n",
    "    database_=DATABASE,\n",
    "    routing_=RoutingControl.READ,\n",
    "    result_transformer_= lambda r: r.to_df()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdTfdAyV2ZaR"
   },
   "outputs": [],
   "source": [
    "schema_result_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents with GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets create a Retrieval agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=LLM, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=EMBEDDINGS_MODEL,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 1 - Retrieve Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that retrieves products from the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_products() -> pd.DataFrame:\n",
    "    \"\"\"Retrieve the products in the database. Products are specified with name. \"\"\"\n",
    "    return driver.execute_query(\n",
    "        \"\"\"\n",
    "        MATCH (p:ProductType)\n",
    "        RETURN p.name as name\n",
    "        \"\"\",\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        result_transformer_= lambda r: r.to_df(),\n",
    "    )['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_products()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 2 - Map Products to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To map a product from some a question we need to map it to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_products_prompt = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to map a product name to product names in the database.\n",
    "\n",
    "Examples:\n",
    "#####\n",
    "Product: savings account. \n",
    "Database Products: ['SpaarRekening', 'DirectRekening', 'Kortlopende Reis', 'BeleggersRekening', 'RaboBusiness Banking']\n",
    "Assistant: Product: SpaarRekening\n",
    "#####\n",
    "#####\n",
    "Product: Direct Rekening. \n",
    "Database Products: ['SpaarRekening', 'DirectRekening', 'Kortlopende Reis', 'BeleggersRekening', 'RaboBusiness Banking']Assistant: Customer: Jan Blok\n",
    "Assistant: Product: DirectRekening\n",
    "\n",
    "#####\n",
    "#####\n",
    "Product: Reis verzekering. \n",
    "Database Products: ['SpaarRekening', 'DirectRekening', 'Kortlopende Reis', 'BeleggersRekening', 'RaboBusiness Banking']Assistant: Customer: Jan Blok\n",
    "Assistant: Product: Kortlopende Reis\n",
    "#####\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_product_to_database_products(product) -> str:\n",
    "    \"\"\"Map products from the user question to the actual products in the database.\"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=LLM,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": map_products_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"Product: \" + product},\n",
    "            {\"role\": \"user\", \"content\": \"Database Products: \" + str(retrieve_products())},\n",
    "            \n",
    "        ],\n",
    "#        response_format=DefinitionList,\n",
    "    )\n",
    "    return response.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_product_to_database_products('savings account')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 3 - Retrieve product documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that retrieves a document from a specific product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_document_from_product(product_name) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve the documents of products in the database. Products are specified with their name. \"\"\"\n",
    "    return driver.execute_query(\n",
    "        \"\"\"\n",
    "        MATCH (p:ProductType)<-[:RELATED_TO]-(d:Document)\n",
    "        WHERE LOWER(p.name) = LOWER($product_name)\n",
    "        RETURN d.file_name\n",
    "        \"\"\",\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        product_name = product_name,\n",
    "        result_transformer_= lambda r: r.to_df(),\n",
    "    ).iloc[0]['d.file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_document_from_product('SpaarRekening')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 4 - GraphRAG Document Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs GraphRAG but only on a specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_graphrag(search_prompt, document):\n",
    "    query_vector = embedding_model.embed_query(search_prompt)\n",
    "\n",
    "    similarity_query = \"\"\" \n",
    "        CALL db.index.vector.queryNodes(\"chunk-embeddings\", 30, $query_vector) YIELD node, score\n",
    "        WITH node as chunk, score ORDER BY score DESC\n",
    "        CALL (chunk) {\n",
    "            MATCH (chunk)-[r:OVERLAPPING_DEFINITIONS]-(overlapping_chunk:Chunk)\n",
    "            WHERE r.overlap > 3\n",
    "            RETURN collect(overlapping_chunk) AS overlapping_chunks\n",
    "        }\n",
    "        WITH [chunk] + overlapping_chunks AS chunks\n",
    "        UNWIND chunks as chunk\n",
    "        MATCH (d:Document{file_name: $document})<-[:PART_OF]-(chunk)\n",
    "        RETURN d.file_name as file_name, chunk.id as chunk_id, chunk.page as page, chunk.chunk_eng AS chunk\n",
    "       \"\"\"\n",
    "    results_1 = driver.execute_query(\n",
    "        similarity_query,\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        query_vector=query_vector,\n",
    "        document=document,\n",
    "        result_transformer_= lambda r: r.to_df()\n",
    "    )\n",
    "\n",
    "    chunk_ids = list(set(results_1['chunk_id'].to_list()))\n",
    "\n",
    "    definition_query = \"\"\"    \n",
    "       CALL db.index.vector.queryNodes(\"definition-embeddings\", 5, $query_vector) YIELD node, score\n",
    "            WITH node as definition, score ORDER BY score DESC\n",
    "            WHERE definition.degree < 20\n",
    "            WITH definition LIMIT 1\n",
    "            MATCH (definition)<-[:MENTIONS]-(chunk:Chunk)\n",
    "            WHERE NOT (chunk.id IN $chunk_ids)\n",
    "            WITH chunk LIMIT 3\n",
    "            MATCH (d:Document{file_name: $document})<-[:PART_OF]-(chunk)\n",
    "            RETURN d.file_name as file_name, chunk.id as chunk_id, chunk.page as page, chunk.chunk_eng AS chunk\n",
    "    \"\"\"\n",
    "    results_2 = driver.execute_query(\n",
    "        definition_query,\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        chunk_ids=chunk_ids,\n",
    "        document=document,\n",
    "        query_vector=query_vector,\n",
    "        result_transformer_= lambda r: r.to_df()\n",
    "    )\n",
    "\n",
    "    results = pd.concat([results_1,results_2]).drop_duplicates()\n",
    "    results = results.to_json(orient=\"records\")\n",
    "    parsed = loads(results)\n",
    "    context = dumps(parsed, indent=2)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(search_prompt, context):\n",
    "    prompt_template = \"\"\"\n",
    "\n",
    "    You are a chatbot on Rabobank product. Your goal is to help people with questions on product policies.  \n",
    "    A user will come to you with questions on their policy. Their questions must be answered based on the relevant documents of the policy.\n",
    "    Respond in English. \n",
    "\n",
    "    The question is the following: \n",
    "    {search_prompt}\n",
    "    \n",
    "    Always respond in the language in which the question was asked. So, do not respond in a different language.\n",
    "    \n",
    "    The context is the following: \n",
    "    {context}\n",
    "\n",
    "    Please explain your answer as thorough as possbile based on the context above. Don't come up with anything yourself.\n",
    "    \n",
    "    Please end your message with listing your sources with file name and page number. \n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    \n",
    "    theprompt = prompt.format_prompt(search_prompt=search_prompt, context=context)\n",
    "    return theprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = get_context_graphrag(\"What are the rules for shared savings account?\", \"Rabo SpaarRekening 2020.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search_in_document(document, search_prompt) -> [str, str]:\n",
    "    \"\"\"Peform a search in the document to search relevant text to answer a user question. The document first needs to be determined before a search should be performed.\"\"\"\n",
    "    context = get_context_graphrag(document, search_prompt)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_in_document(question, document) -> str:\n",
    "    \"\"\"This function is answering a question based on a search in a document (vector search on document). Document and question both need to be provided.\"\"\"\n",
    "    context = perform_search_in_document(question, document)\n",
    "    theprompt = generate_prompt(question, context)\n",
    "    return llm(theprompt.to_messages()).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = answer_question_in_document(\"What are the rules for shared savings account?\", \"Rabo SpaarRekening 2020.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 5 - Retrieve products from Customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that retrieves the products from a customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_products_of_customers(customer_id) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve the products of a customer in the database. Customers are specified with their id. \"\"\"\n",
    "    return driver.execute_query(\n",
    "        \"\"\"\n",
    "        MATCH (c:Customer)-[:HAS_PRODUCT]->(p:Product)\n",
    "        WHERE c.id = $customer_id\n",
    "        RETURN p.id as product_id, p.name as product_name\n",
    "        \"\"\",\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        customer_id = customer_id,\n",
    "        result_transformer_= lambda r: r.to_df(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_products_of_customers(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 6 - Retrieve information from a product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function retrieving all properties of a product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_information_from_product(product_id) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve the information of a product in the database. Product is specified with id.\"\"\"\n",
    "    result = driver.execute_query(\n",
    "        \"\"\"\n",
    "       MATCH (p:Product{id: $product_id})\n",
    "        RETURN properties(p) as props\n",
    "        \"\"\",\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        product_id = product_id,\n",
    "        result_transformer_= lambda r: r.to_df(),\n",
    "    )\n",
    "    return result.iloc[0]['props']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_information_from_product(\"ef31587f-5c96-4ab2-99e6-99f3b6fa88e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 7 - Retrieve Customer ID based on full name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve customer id from the database based on name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_customer_id_from_database_based_on_full_name(full_name) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve customers from the database database based on full_name. customer_id is returned.\"\"\"\n",
    "    \n",
    "    query = \"\"\"MATCH (c:Customer) WHERE c.name = $name RETURN c.id\"\"\"\n",
    "\n",
    "    result = driver.execute_query(\n",
    "        query,\n",
    "        database_=DATABASE,\n",
    "        routing_=RoutingControl.READ,\n",
    "        name = full_name,\n",
    "        result_transformer_= lambda r: r.to_df(),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_customer_id_from_database_based_on_full_name(\"Lucas Van den Berg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=LLM, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke([HumanMessage(content=\"hi!\")])\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    retrieve_products,\n",
    "    map_product_to_database_products,\n",
    "    retrieve_document_from_product,\n",
    "    answer_question_in_document,\n",
    "    retrieve_products_of_customers,\n",
    "    retrieve_information_from_product,\n",
    "    retrieve_customer_id_from_database_based_on_full_name,\n",
    "]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Agents with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating a simple [react agent](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) using Langgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"hi!\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what Tool will be called on which question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke([HumanMessage(content=\"What products does Jan Kok have?\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke([HumanMessage(content=\"What is my expiration date of my savings account? My name is Jan Kok\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke([HumanMessage(content=\"I got a question on my savings account\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke([HumanMessage(content=\"I got a question on my savings account, what are the rules for a joint account?\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run some examples! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some examples with the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_to_agent(question):\n",
    "    for step in agent_executor.stream(\n",
    "        {\"messages\": [HumanMessage(content=question)]},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What Products does Jan Kok have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_to_agent(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I got a question on my savings account, what are the rules for a joint account?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_to_agent(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When is my travel insurance exprired? My name is Daan Visser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_to_agent(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When is my IBAN of my Saving account? My name Lucas van den Berg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_to_agent(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a chatbot with the agent providing the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user(user_message, history):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    return \"\", history\n",
    "\n",
    "def get_answer(history):\n",
    "    steps = []\n",
    "    full_prompt = \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in history])\n",
    "    \n",
    "    for step in agent_executor.stream(\n",
    "            {\"messages\": [HumanMessage(content=full_prompt)]},\n",
    "            stream_mode=\"values\",\n",
    "    ):\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "        steps.append(step[\"messages\"][-1].content)\n",
    "    \n",
    "    return steps[-1]\n",
    "\n",
    "def bot(history):\n",
    "    bot_message = get_answer(history)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "\n",
    "    for character in bot_message:\n",
    "        history[-1][\"content\"] += character\n",
    "        time.sleep(0.01)\n",
    "        yield history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"Chatbot on a Graph\",\n",
    "        avatar_images=[\n",
    "            \"https://png.pngtree.com/png-vector/20220525/ourmid/pngtree-concept-of-facial-animal-avatar-chatbot-dog-chat-machine-illustration-vector-png-image_46652864.jpg\",\n",
    "            \"https://d-cb.jc-cdn.com/sites/crackberry.com/files/styles/larger/public/article_images/2023/08/openai-logo.jpg\"\n",
    "        ],\n",
    "        type=\"messages\", \n",
    "    )\n",
    "    msg = gr.Textbox(label=\"Message\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, [chatbot], chatbot\n",
    "    )\n",
    "\n",
    "    clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have the light-mode for the chatbot paste the following after the URL: /?__theme=light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within LangGraph you can also define [Agent Workflows](https://langchain-ai.github.io/langgraph/tutorials/workflows/). Below we are quickly setting up such a workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_product_customer_prompt = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to find either a customer name or product from the text submitted. \n",
    "The goal is to retrieve either the product on which someone is asking a question. Or the customer name such that we can find the products from their. \n",
    "Please only return the product name that is extracted. \n",
    "\n",
    "Examples:\n",
    "#####\n",
    "User: I got a question about my savings account. \n",
    "Assistant: Product: savings account\n",
    "#####\n",
    "#####\n",
    "User: My name is Jan Blok and I got a question. \n",
    "Assistant: Customer: Jan Blok\n",
    "#####\n",
    "#####\n",
    "User: What is the policy on account? \n",
    "Assistant: Need more information\n",
    "#####\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_product_or_customer_from_text(question):\n",
    "    \"\"\"Retrieve either products or customers from the text given by the user\"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=LLM,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": retrieve_product_customer_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "#        response_format=DefinitionList,\n",
    "    )\n",
    "    return response.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    input: str\n",
    "    question: str\n",
    "    product: str\n",
    "    customer: str\n",
    "    document: str\n",
    "    answer: str\n",
    "\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "# no-op node that should be interrupted on\n",
    "def human_feedback(state: State):\n",
    "    pass\n",
    "\n",
    "\n",
    "def retrieve_product_customer(state):\n",
    "    result = retrieve_product_or_customer_from_text(state['input'])\n",
    "    if 'Customer: ' in result:\n",
    "        customer = result.split('Customer: ')[1]\n",
    "        print('It looks like you are the following customer: ' + customer)\n",
    "        state['customer'] = customer\n",
    "    elif 'Product: ' in result:\n",
    "        product = result.split('Product: ')[1]\n",
    "        print('It sounds like you got a question on the following product: ' + product)\n",
    "        state['product'] = product\n",
    "    return state\n",
    "\n",
    "def user_feedback(state):\n",
    "    print(\"I couldn't find information to start the flow. Could you specify a product on which you have questions or your customer name?\")\n",
    "    return state \n",
    "    \n",
    "def map_product_to_db(state):\n",
    "    result = map_product_to_database_products(state['product'])\n",
    "    product = result.split('Product: ')[1]\n",
    "    state['product'] = product\n",
    "    print(\"I have found the following product in the database: \" + product)\n",
    "    return state\n",
    "\n",
    "def retrieve_document(state):\n",
    "    result = retrieve_document_from_product(state['product'])\n",
    "    state['document'] = result\n",
    "    print(\"I found the following document on the product: \" + result)\n",
    "    return state\n",
    "\n",
    "def tools_condition(state) -> Literal[\"user_feedback\", \"map_product\"]: \n",
    "    if (state.get('product') is None):\n",
    "        return \"user_feedback\"\n",
    "    else: \n",
    "        return \"map_product\"\n",
    "\n",
    "def answer_question(state):\n",
    "    context = perform_search_in_document(state['question'], state['document'])\n",
    "    theprompt = generate_prompt(state['question'], context)\n",
    "    state['answer'] = llm(theprompt.to_messages()).content\n",
    "    return state \n",
    "    \n",
    "# Build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"retrieve_product_customer\", retrieve_product_customer)\n",
    "builder.add_node(\"user_feedback\", user_feedback)\n",
    "builder.add_node(\"map_product\", map_product_to_db)\n",
    "builder.add_node(\"retrieve_document\", retrieve_document)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"answer_question\", answer_question)\n",
    "\n",
    "# Logic\n",
    "builder.add_edge(START, \"retrieve_product_customer\")\n",
    "builder.add_conditional_edges(\"retrieve_product_customer\", tools_condition)\n",
    "builder.add_edge(\"user_feedback\", END)\n",
    "builder.add_edge(\"map_product\", \"retrieve_document\")\n",
    "builder.add_edge(\"retrieve_document\", \"human_feedback\")\n",
    "builder.add_edge(\"human_feedback\", \"answer_question\")\n",
    "builder.add_edge(\"answer_question\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Add\n",
    "graph = builder.compile(checkpointer=memory, interrupt_before=[\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input = {\"input\" : \"I got a question on my savings account, what are the rules for a shared account?\"}\n",
    "\n",
    "thread  = {\"configurable\": {\"thread_id\": \"25\"}}\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "print(graph.get_state(thread))\n",
    "\n",
    "user_input = input(f\"Please raise your question on {graph.get_state(thread).values['product']}: \")\n",
    "\n",
    "graph.update_state(thread, {\"question\": user_input}, as_node=\"human_feedback\")\n",
    "\n",
    "graph.get_state(thread).next\n",
    "\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(thread).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.get_state(thread).values['answer'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
